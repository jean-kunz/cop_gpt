{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/learn/cookbook/fine_tuning_llm_grpo_trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee0834355564b7d8fa94da01b46584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "\n",
    "\n",
    "# use notebook key. Paste with menu: Edit->paste in vscode.\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bdfca0f2dd4d3b9e8d547461801ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "# dataset_id = \"AI-MO/NuminaMath-TIR\"\n",
    "# dataset_id = \"openai/gsm8k\"\n",
    "# train_dataset, test_dataset = load_dataset(\n",
    "#    dataset_id, split=[\"train[:5%]\", \"test[:5%]\"]\n",
    "# )\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split=\"train\") -> Dataset:\n",
    "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]  # type: ignore\n",
    "    data = data.map(\n",
    "        lambda x: {  # type: ignore\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "            \"solution\": x[\"answer\"],\n",
    "        }\n",
    "    )  # type: ignore\n",
    "    return data  # type: ignore\n",
    "\n",
    "\n",
    "train_dataset = get_gsm8k_questions(split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': '72',\n",
       " 'prompt': [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "   'role': 'user'}],\n",
       " 'solution': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " [Union({1, 3}, {2, 4}), '{1,3} \\\\cup {2,4}'],\n",
       " [{1, 2, 3, 4}, '{1,2,3,4}'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemple of maths verify\n",
    "\n",
    "# \\\\cup : union of two sets\n",
    "gold = parse(\"${1,3} \\\\cup {2,4}$\")\n",
    "answer = parse(\"${1,2,3,4}$\")\n",
    "\n",
    "# Order here is important!\n",
    "verify(gold, answer), gold, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content) for content in completion_contents]\n",
    "    rewards_list = [1.0 if match else 0.0 for match in matches]\n",
    "    return rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n",
    "]\n",
    "completions = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\",\n",
    "        }\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\",\n",
    "        }\n",
    "    ],\n",
    "]\n",
    "format_reward(prompts=prompts, completions=completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    q = prompts[0][-1][\"content\"]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(\n",
    "        \"-\" * 20,\n",
    "        f\"Question:\\n{q}\",\n",
    "        f\"\\nAnswer:\\n{answer[0]}\",\n",
    "        f\"\\nResponse:\\n{responses[0]}\",\n",
    "        f\"\\nExtracted:\\n{extracted_responses[0]}\",\n",
    "    )\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m\n\u001b[1;32m     27\u001b[0m completion_test \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     [\n\u001b[1;32m     29\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     34\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m accuracy_reward(\n\u001b[1;32m     36\u001b[0m     completions\u001b[38;5;241m=\u001b[39mcompletion_test,\n\u001b[1;32m     37\u001b[0m     solution\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m72\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m1.0\u001b[39m]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m accuracy_reward(\n\u001b[1;32m     40\u001b[0m     completions\u001b[38;5;241m=\u001b[39mcompletion_test,\n\u001b[1;32m     41\u001b[0m     solution\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m75\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    solutions = kwargs[\"solution\"]\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content, solution in zip(completion_contents, solutions):\n",
    "        gold_parsed = parse(\n",
    "            solution,\n",
    "            extraction_mode=\"first_match\",\n",
    "            extraction_config=[LatexExtractionConfig()],\n",
    "        )\n",
    "        answer_parsed = parse(\n",
    "            content,\n",
    "            extraction_mode=\"first_match\",\n",
    "            extraction_config=[LatexExtractionConfig()],\n",
    "        )\n",
    "        if len(gold_parsed) != 0:\n",
    "            try:\n",
    "                rewards.append(float(verify(answer_parsed, gold_parsed)))\n",
    "            except Exception:\n",
    "                rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "completion_test = [\n",
    "    [\n",
    "        {\n",
    "            \"content\": \"\"\"<think>Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.</think><answer>72</answer>\"\"\",\n",
    "            \"role\": \"assistant\",\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "assert accuracy_reward(\n",
    "    completions=completion_test,\n",
    "    solution=[\"72\"],\n",
    ") == [1.0]\n",
    "assert accuracy_reward(\n",
    "    completions=completion_test,\n",
    "    solution=[\"75\"],\n",
    ") == [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given that the function $f(x)=\\sin (π-ωx)\\cos ωx+\\cos ^{2}ωx\\ (ω > 0)$ has a minimum positive period of $π$.\n",
      "(I) Find the value of $ω$;\n",
      "(II) The horizontal coordinates of each point on the graph of the function $y=f(x)$ are shortened to half of the original, and the vertical coordinates remain unchanged, resulting in the graph of the function $y=g(x)$. Find the minimum value of the function $y=g(x)$ on the interval $[0, \\frac{π}{16}]$.\n"
     ]
    }
   ],
   "source": [
    "ex_idx = 100\n",
    "print(train_dataset[\"prompt\"][ex_idx][1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, let's break it down step by step and use Python and sympy as needed.\n",
      "\n",
      "### Part (I): Find the value of \\( \\omega \\)\n",
      "To find \\( \\omega \\) such that the function \\( f(x) = \\sin(\\pi - \\omega x) \\cos(\\omega x) + \\cos^2(\\omega x) \\) has a minimum positive period of \\( \\pi \\), we need to determine when the function repeats itself after \\( \\pi \\) units.\n",
      "\n",
      "First, let's simplify the function \\( f(x) \\):\n",
      "\\[\n",
      "f(x) = \\sin(\\pi - \\omega x) \\cos(\\omega x) + \\cos^2(\\omega x)\n",
      "\\]\n",
      "Using the trigonometric identities, \\( \\sin(\\pi - \\theta) = \\sin(\\theta) \\) and \\( \\cos^2(\\theta) = 1 - \\sin^2(\\theta) \\):\n",
      "\\[\n",
      "f(x) = \\sin(\\omega x) \\cos(\\omega x) + \\cos^2(\\omega x)\n",
      "\\]\n",
      "\\[\n",
      "f(x) = \\frac{1}{2}\\sin(2\\omega x) + \\cos^2(\\omega x)\n",
      "\\]\n",
      "\n",
      "Given that the period of \\(f(x)\\) is \\( \\pi \\), we know that:\n",
      "\\[\n",
      "f(x + \\pi) = f(x)\n",
      "\\]\n",
      "\n",
      "For \\(f(x)\\) to have a period of \\(\\pi\\), both \\(\\sin(2\\omega x)\\) and \\(\\cos^2(\\omega x)\\) must repeat after \\(\\pi\\). The period of \\(\\sin(2\\omega x)\\) is \\(\\frac{\\pi}{2\\omega}\\). For it to match, the period must be \\(\\pi\\):\n",
      "\\[\n",
      "\\frac{\\pi}{2\\omega} = \\pi \\implies \\omega = \\frac{1}{2}\n",
      "\\]\n",
      "\n",
      "Let's verify this value by evaluating the sympy function.\n",
      "\n",
      "### Part (II): Finding the Minimum Value of \\( g(x) \\)\n",
      "When the horizontal coordinates are halved, the new function \\( g(x) \\) is:\n",
      "\\[\n",
      "g(x) = f(2x) = \\sin(\\pi - \\omega (2x)) \\cos(\\omega (2x)) + \\cos^2(\\omega (2x))\n",
      "\\]\n",
      "\n",
      "Using \\( \\omega = \\frac{1}{2} \\):\n",
      "\\[\n",
      "g(x) = \\sin(\\pi - x) \\cos(x) + \\cos^2(x)\n",
      "\\]\n",
      "\n",
      "We are asked to find the minimum value of \\( g(x) \\) on the interval \\([0, \\frac{\\pi}{16}]\\).\n",
      "\n",
      "Let's solve this using Python and sympy.\n",
      "\n",
      "```python\n",
      "import sympy as sp\n",
      "\n",
      "# Define the variable and the function\n",
      "x = sp.symbols('x')\n",
      "omega = 1/2\n",
      "f = sp.sin(sp.pi - omega * 2 * x) * sp.cos(omega * 2 * x) + sp.cos(omega * 2 * x)**2\n",
      "\n",
      "# Simplify the function g(x)\n",
      "g = sp.simplify(f)\n",
      "\n",
      "# Find the critical points by taking the derivative and solving for 0\n",
      "dg = sp.diff(g, x)\n",
      "critical_points = sp.solveset(dg, x, domain=sp.Interval(0, sp.pi/16))\n",
      "\n",
      "# Evaluate g(x) at the critical points and endpoints\n",
      "min_value = sp.Min(g.subs(x, 0), g.subs(x, sp.pi/16))\n",
      "for point in critical_points:\n",
      "    min_value = sp.Min(min_value, g.subs(x, point))\n",
      "\n",
      "min_value = sp.N(min_value)\n",
      "print(min_value)\n",
      "```\n",
      "```output\n",
      "1.00000000000000\n",
      "```\n",
      "The minimum value of the function \\( g(x) \\) on the interval \\([0, \\frac{\\pi}{16}]\\) is \\(\\boxed{1}\\).\n",
      "\n",
      "Here's the summarized solution:\n",
      "\n",
      "### Part (I):\n",
      "We found that the value of \\( \\omega \\) such that the function \\( f(x) = \\sin(\\pi - \\omega x) \\cos(\\omega x) + \\cos^2(\\omega x) \\) has a minimum positive period of \\( \\pi \\) is:\n",
      "\\[ \\omega = \\frac{1}{2} \\]\n",
      "\n",
      "### Part (II):\n",
      "After transforming the function by halving the horizontal coordinates, we derived the new function \\( g(x) \\). We found that the minimum value of \\( g(x) \\) on the interval \\([0, \\frac{\\pi}{16}]\\) is:\n",
      "\\[ \\boxed{1} \\]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"solution\"][ex_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'content': ['system\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\\nuser\\nWhat is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.\\nuser']}, {'content': ['system\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\\nuser\\nWhat is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.\\nuser']}, {'content': ['system\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\\nuser\\nWhat is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.\\nedited:\\n\\nThe coefficient of \\\\(x^2y^6\\\\) in the expansion of \\\\']}]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 73\u001b[0m\n\u001b[1;32m     63\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m     64\u001b[0m     model,\n\u001b[1;32m     65\u001b[0m     ref_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# formatted_text = tokenizer.apply_chat_template(train_dataset[0][\"prompt\"], tokenize=False)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 42\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[0;34m(self, questions, solutions)\u001b[0m\n\u001b[1;32m     40\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, questions)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(completions)\n\u001b[0;32m---> 42\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_advantage(rewards)\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(advantages, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m )\n",
      "Cell \u001b[0;32mIn[30], line 32\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_rewards\u001b[0;34m(self, completions, solutions)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_rewards\u001b[39m(\u001b[38;5;28mself\u001b[39m, completions, solutions):\n\u001b[0;32m---> 32\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(rewards)\n",
      "Cell \u001b[0;32mIn[30], line 57\u001b[0m, in \u001b[0;36mcombined_rewards\u001b[0;34m(completions, solutions)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcombined_rewards\u001b[39m(completions, solutions):\n\u001b[0;32m---> 57\u001b[0m     format_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mformat_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     accuracy_rewards \u001b[38;5;241m=\u001b[39m accuracy_reward(completions, solutions)\n\u001b[1;32m     59\u001b[0m     combined_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m format_rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m accuracy_rewards\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mformat_reward\u001b[0;34m(completions, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reward function that checks if the completion has a specific format.\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^<think>.*?</think>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*<answer>.*?</answer>$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m completion_contents \u001b[38;5;241m=\u001b[39m [\u001b[43mcompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m      8\u001b[0m matches \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39mmatch(pattern, content) \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m completion_contents]\n\u001b[1;32m      9\u001b[0m rewards_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, model, ref_model, reward_model, tokenizer, device, lr=1e-5):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.reward_model = reward_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "\n",
    "    def sample_outputs(self, model, prompts, num_samples=3):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(prompts, tokenize=False)\n",
    "        inputs = self.tokenizer(\n",
    "            formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(self.device)\n",
    "        outputs = [model.generate(**inputs) for _ in range(num_samples)]\n",
    "        completions = [\n",
    "            [\n",
    "                {\"content\": self.tokenizer.batch_decode(out, skip_special_tokens=True)}\n",
    "                for out in outputs\n",
    "            ]\n",
    "        ]\n",
    "        return completions\n",
    "\n",
    "    def compute_rewards(self, completions, solutions):\n",
    "        rewards = [self.reward_model(c, solutions) for c in completions]\n",
    "        return np.array(rewards)\n",
    "\n",
    "    def compute_advantage(self, rewards):\n",
    "        baseline = np.mean(rewards)\n",
    "        return rewards - baseline\n",
    "\n",
    "    def train_step(self, questions, solutions):\n",
    "        completions = self.sample_outputs(self.model, questions)\n",
    "        print(completions)\n",
    "        rewards = self.compute_rewards(completions, solutions)\n",
    "        advantages = self.compute_advantage(rewards)\n",
    "\n",
    "        loss = -torch.mean(\n",
    "            torch.tensor(advantages, dtype=torch.float32, requires_grad=True)\n",
    "        )\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "def combined_rewards(completions, solutions):\n",
    "    format_rewards = format_reward(completions)\n",
    "    accuracy_rewards = accuracy_reward(completions, solutions)\n",
    "    combined_rewards = 0.5 * format_rewards + 0.5 * accuracy_rewards\n",
    "    return combined_rewards\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model,\n",
    "    ref_model=model,\n",
    "    reward_model=combined_rewards,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "# formatted_text = tokenizer.apply_chat_template(train_dataset[0][\"prompt\"], tokenize=False)\n",
    "loss = trainer.train_step(train_dataset[0][\"prompt\"], [train_dataset[0][\"solution\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].get(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions = [\n",
    "    \"system\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\\nuser\\nWhat is the coefficient of $x^2y^6$ in the expansion of $\\\\left(\\\\frac{3}{5}x-\\\\frac{y}{2}\\\\right)^8$?  Express your answer as a common fraction.\\narterms = 1 + 6 - 8\\ncoefficient = terms * (terms\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"prompt\"], tokenize=False\n",
    ")\n",
    "formatted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(formatted_text)\n",
    "train_dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    for sample in train_dataset:\n",
    "        formatted_text = tokenizer.apply_chat_template(sample[\"prompt\"], tokenize=False)\n",
    "        loss = trainer.train_step([formatted_text], [sample[\"solution\"]])\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen2-0.5B-GRPO-test\",\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    # Parameters that control de data preprocessing\n",
    "    max_completion_length=64,  # default: 256\n",
    "    num_generations=4,  # default: 8\n",
    "    max_prompt_length=128,  # default: 512\n",
    "    # Parameters related to reporting and saving\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=10,\n",
    "    push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[format_reward, accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "do_train = False\n",
    "if do_train:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    trainer.save_model(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sergiopaniego/Qwen2-0.5B-GRPO\"\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate_with_reasoning(prompt, r_model, r_tokenizer):\n",
    "    # Build the prompt from the dataset\n",
    "    prompt = \" \".join(entry[\"content\"] for entry in prompt)\n",
    "\n",
    "    # Tokenize and move to the same device as the model\n",
    "    inputs = r_tokenizer(prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "\n",
    "    # Generate text without gradients\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = r_model.generate(**inputs, max_length=500)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Decode and extract model response\n",
    "    generated_text = r_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get inference time\n",
    "    inference_duration = end_time - start_time\n",
    "\n",
    "    # Get number of generated tokens\n",
    "    num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "    num_generated_tokens = output_ids.shape[1] - num_input_tokens\n",
    "    response_text = generated_text[len(prompt) :].strip()\n",
    "\n",
    "    return response_text, generated_text, inference_duration, num_generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = test_dataset[\"prompt\"][0]\n",
    "response_text, generated_text, inference_duration, num_generated_tokens = (\n",
    "    generate_with_reasoning(prompt, trained_model, trained_tokenizer)\n",
    ")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw model without reasoning training\n",
    "response_text, generated_text, inference_duration, num_generated_tokens = (\n",
    "    generate_with_reasoning(prompt, model, trained_tokenizer)\n",
    ")\n",
    "\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----\")\n",
    "# print(\"answer:\", test_dataset[0][\"answer\"])\n",
    "print(test_dataset[0][\"solution\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
