{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download:bool=False\n",
    "if do_download:\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different methods to compute rolling mean on T axis.\n",
    "like weighted sum with same weight on previous items of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "xbow = torch.zeros((B, T, C))\n",
    "xbow.shape\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]\n",
    "        # print(xprev)\n",
    "        # print(b, t, xprev.shape)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "        # print(xbow[b, t])\n",
    "        # print(\"-----\")\n",
    "        # print(xbow)\n",
    "\n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "print(wei)\n",
    "wei = wei / wei.sum(axis=1, keepdim=True)\n",
    "print(\"weight\", wei)\n",
    "xbow2 = wei @ x  # B,T,C @ B,T,T ---> B,T,C\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3 using softmax\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "\n",
    "wei = torch.zeros((T, T))\n",
    "# print(wei)\n",
    "wei = wei.masked_fill(tril == 0, value=float(\"-inf\"))\n",
    "# print(wei)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(\"weight\", wei)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 16, 8, 32\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "head_size = 32\n",
    "key = nn.Linear(C, head_size)\n",
    "k = key(x)\n",
    "query = nn.Linear(C, head_size)\n",
    "q = query(x)\n",
    "value = nn.Linear(C, head_size)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose vs reshape\n",
    "\n",
    "m = torch.tensor(range(1, 65))\n",
    "print(m)\n",
    "m.storage()\n",
    "b, t, c = 2, 8, 4\n",
    "mr = m.reshape(b, t, c)\n",
    "# mr.storage()\n",
    "print(mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = m.view(b, t, c)\n",
    "mv.view(b, c, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you transpose axis not shape dimensions\n",
    "mr.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor(range(1, 11)).reshape(2, 5)\n",
    "print(s)\n",
    "print(s.transpose(0, 1))\n",
    "print(s.transpose(1, 0))\n",
    "st = s.transpose(1, 0)\n",
    "print(st.storage())\n",
    "# you change the physical storage of the tensor to be contiguous\n",
    "st.contiguous().storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep\n",
    "\n",
    "- BPE tokenizer\n",
    "- character based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(text))\n",
    "train_txt = text[:n]\n",
    "test_txt = text[n:]\n",
    "train_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "# GPT doen't use a normalizer but we add it here for demo purposes\n",
    "normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "normalizer.normalize_str(train_txt[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, (s, e) in tokenizer.pre_tokenizer.pre_tokenize_str(train_txt[:50]):\n",
    "    print(t, s, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=1000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator([train_txt], trainer=trainer)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "encodings = tokenizer.encode(train_txt[:50])\n",
    "print(\"tokens: \", encodings.tokens)\n",
    "print(\"ids: \", encodings.ids)\n",
    "print(\"decoded:\", tokenizer.decode(encodings.ids))\n",
    "# train_bpe = tokenizer.encode(train_txt).ids\n",
    "# test_bpe = tokenizer.encode(test_txt).ids\n",
    "print(f\"offsets:{[train_txt[s:e] for s,e in encodings.offsets]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation data for bpe based tokens\n",
    "data = torch.tensor(tokenizer.encode(text).ids, dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_bpe = data[:n]\n",
    "val_bpe = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_batch(split: str = \"train\", batch_size: int = 16, block_size: int = 8):\n",
    "    data = train_bpe if split == \"train\" else val_bpe\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    ic(ix)\n",
    "    # ix are random starting points of each batch\n",
    "\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "get_bpe_batch(batch_size=4, block_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    x, y = get_bpe_batch(batch_size=16, block_size=8)\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.to(int).tolist())\n",
    "    print(tokenizer.decode_batch(x.to(int).tolist()))\n",
    "    print(tokenizer.decode_batch(y.to(int).tolist()))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for i, s in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "def decode(l: list[int]) -> str:\n",
    "    return \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(encode(\"j'aime les chats\"))\n",
    "# stoi[\"a\"], stoi[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Compute train and validation data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(text))\n",
    "train = data[:n]\n",
    "val = data[n:]\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_char_batch(split: str = \"train\", batch_size: int = 16, block_size: int = 8):\n",
    "    data = train if split == \"train\" else val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    # ix are random starting points of each batch\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_char_batch(batch_size=16, block_size=8)\n",
    "x, y\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_bpe: bool = False\n",
    "if use_bpe:\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    get_batch = get_bpe_batch\n",
    "else:\n",
    "    vocab_size = 65\n",
    "    get_batch = get_char_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size: int, n_embd: int, block_size: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.smax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"tril\", tensor=torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = self.smax(wei)  # F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, head_size: int, block_size: int, n_embd: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(head_size, block_size=block_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), nn.ReLU(), nn.Linear(4 * n_embd, n_embd), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, num_heads: int, block_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads, head_size=head_size, block_size=block_size, n_embd=n_embd, dropout=dropout\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embd, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_heads: int = 4,\n",
    "        n_embd: int = 32,\n",
    "        block_size: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "        self.num_layers = num_layers\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = torch.nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, block_size, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = torch.nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=device))\n",
    "        # print(\"pos_emb shape\", pos_emb.shape)\n",
    "        x = tok_emb + pos_emb  # B, T, C\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # print(f\"logits shape: {logits.shape}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size :]\n",
    "            logits, _ = self(idx_cond)\n",
    "            # print(\"generate logit shape\", logits.shape)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "# hyperparams\n",
    "batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "block_size = 256  # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "n_embd = 384\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size, num_heads=num_heads, num_layers=num_layers, n_embd=n_embd, block_size=block_size, dropout=dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "xb, yb = get_batch(\"train\")\n",
    "train_logit, train_loss = model(xb)\n",
    "train_logit.shape\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "print(\"it should be about 10M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=batch_size, block_size=block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "max_iters = 7001\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model_id = \"cop_gpt\"\n",
    "model_version = \"0.1\"\n",
    "log_dir = Path(\"../runs\") / f\"{model_id}-{model_version}\" / f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "do_train: bool = False\n",
    "last_iter = 0  # to start from scratch\n",
    "last_iter = 5000\n",
    "if last_iter > 0:\n",
    "    model = torch.load(f\"gpt-{model_version}-{last_iter}.pt\")\n",
    "if do_train:\n",
    "    for i in range(last_iter + 1, max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if i % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            writer.add_scalar(\"loss/train\", losses[\"train\"], i)\n",
    "            writer.add_scalar(\"loss/val\", losses[\"val\"], i)\n",
    "            torch.save(model, f\"gpt-{model_version}-{i}.pt\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(\"train\", batch_size=batch_size, block_size=block_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        for name, weight in model.named_parameters():\n",
    "            writer.add_histogram(name, weight, i)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model, f\"gpt-{model_version}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not do_train:\n",
    "    model = torch.load(f\"gpt-{model_version}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "if use_bpe:\n",
    "    print(tokenizer.decode(model.generate(context, max_new_tokens=50)[0].to(int).tolist()))\n",
    "else:\n",
    "    print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.state_dict()[\"token_emb.weight\"]\n",
    "metadata = [itos[i] for i in range(vocab_size)]\n",
    "\n",
    "\n",
    "# Add embeddings to the writer\n",
    "writer.add_embedding(embeddings, metadata=metadata)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand model behaviour\n",
    "\n",
    "Capture the activation at various moment within the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "# register a hook to access activations of each head.\n",
    "for b, block in enumerate(model.blocks):\n",
    "    for i, h in enumerate(block.sa.heads):\n",
    "        h.register_forward_hook(get_activation(f\"block_{b}_head_{i}\"))\n",
    "        h.smax.register_forward_hook(get_activation(f\"block_{b}_smax_{i}\"))\n",
    "\n",
    "model.lm_head.register_forward_hook(get_activation(\"lm_head\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "gen_toks = model.generate(context, max_new_tokens=8)\n",
    "chars = [c for c in decode(gen_toks[0].tolist())]\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = num_heads // 2\n",
    "num_cols = num_heads // num_rows\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(7, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    im = ax.imshow(activation[f\"block_0_smax_{i}\"][0].cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "    ax.set_title(f\"head {i}\")\n",
    "    ax.set_xticks(np.arange(len(chars)), labels=chars)\n",
    "    ax.set_yticks(np.arange(len(chars)), labels=chars)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=axes, orientation=\"vertical\", fraction=0.1, pad=0.05)\n",
    "\n",
    "\n",
    "# fig.colorbar(im, ax=ax)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogramt (torch.histogram) for each output  distribution for each layer. Also plot gradient to check its normally distributed.\n",
    "for l in model.blocks[0].sa.heads.named_parameters():\n",
    "    print(l[0], l[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_params(model, layer_name):\n",
    "    npp = model.state_dict()[layer_name].cpu().detach().numpy()\n",
    "\n",
    "    plt.imshow(npp, cmap=\"hot\", interpolation=\"nearest\")\n",
    "    plt.title(layer_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_params(model, \"token_emb.weight\")\n",
    "display_params(model, \"pos_emb.weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of experts\n",
    "\n",
    "https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert module\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding how gating/router works\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embed = 32\n",
    "\n",
    "\n",
    "# Example multi-head attention output for a simple illustrative example, consider n_embed=32, context_length=4 and batch_size=2\n",
    "mh_output = torch.randn(2, 4, n_embed)\n",
    "\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts)  # nn.Linear(32, 4)\n",
    "\n",
    "logits = topkgate_linear(mh_output)\n",
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
    "print(logits)\n",
    "\"top k logits:\", top_k_logits, \"top k indices:\", top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the top-k experts and set the rest to -inf\n",
    "zeros = torch.full_like(\n",
    "    logits, float(\"-inf\")\n",
    ")  # full_like clones a tensor and fills it with a specified value (like infinity) for masking or calculations.\n",
    "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
    "# transform the logits into a probability distribution\n",
    "gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "sparse_logits, gating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_ouput):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing this out:\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 32\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices\n",
    "# And it works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softplus is a smoothed version of RELU function.\n",
    "input = torch.tensor([0.2, 2.3, 10.0, -0.1, -3.2, -10.0])\n",
    "sp = F.softplus(input)\n",
    "\n",
    "rand_noise = torch.randn_like(input)\n",
    "out = rand_noise * F.softplus(input)\n",
    "input, sp.numpy().round(4), out.numpy().round(4),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the above to accomodate noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    \"\"\"Essentially, you don't want all the tokens to be sent to the same set of 'favored' experts.\n",
    "    You want a fine balance of exploitation and exploration. For this purpose, to load balance,\n",
    "    it is helpful to add standard normal noise to the logits from the gating linear layer.\n",
    "    This makes training more efficient\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        # layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        # Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        # Adding scaled unit gaussian noise to the logits\n",
    "        # softplus ensures that the noise is always positive and right skewed\n",
    "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
    "        # noisy logit add noise to the logits so some tokens are sent to different experts and not just the top-k.\n",
    "        # It pushes the model to explore more.\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        ic(gating_output)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            ic(expert_mask)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ic.enable()\n",
    "# ic.disable()\n",
    "\n",
    "# Let's test this out\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(1, 8, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view layer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add mechanism to view attention on each token\n",
    "https://www.comet.com/site/blog/explainable-ai-for-transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
